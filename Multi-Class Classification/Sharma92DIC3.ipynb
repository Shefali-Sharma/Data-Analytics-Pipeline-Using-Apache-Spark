{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#NAIVE BAYES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Known Data 40.666666666666664\n",
      "Accuracy 40.909090909090914 on Unknown Data\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.mllib.classification import NaiveBayes, NaiveBayesModel\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "def labelData(line):\n",
    "        probability = [float(row) for row in line.split(' ')]\n",
    "        return LabeledPoint(probability[0], probability[1:])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    #sc = SparkContext(appName=\"Naive-Bayes\")\n",
    "\n",
    "    # Load and parse the data file.\n",
    "    NYT_data = sc.textFile(\"Desktop/SHEF/known_cleaned.txt\")\n",
    "    labeledData = NYT_data.map(labelData)\n",
    "    NYT_data_unknown = sc.textFile(\"Desktop/SHEF/unknown_cleaned.txt\")\n",
    "    unknown_labeledData = NYT_data_unknown.map(labelData)\n",
    "\n",
    "    # Dividing data to training and test data with a ration of 8/2.\n",
    "    training, test = labeledData.randomSplit([0.6, 0.4])\n",
    "\n",
    "    naive_model = NaiveBayes.train(training, 1.0)\n",
    "\n",
    "    # Make prediction and test accuracy.\n",
    "    prediction = test.map(lambda a: (naive_model.predict(a.features), a.label))\n",
    "    efficiency = 1.0 * prediction.filter(lambda b: b[0] == b[1]).count() / test.count()\n",
    "    error = 1 - efficiency\n",
    "    accuracy_percent = efficiency*100\n",
    "    error_percent = error*100\n",
    "    print('Accuracy on Known Data {}'.format(accuracy_percent))\n",
    "    \n",
    "    prediction = unknown_labeledData.map(lambda a: (naive_model.predict(a.features), a.label))\n",
    "    efficiency = 1.0 * prediction.filter(lambda b: b[0] == b[1]).count() / unknown_labeledData.count()\n",
    "    error = 1 - efficiency\n",
    "    accuracy_percent = efficiency*100\n",
    "    error_percent = error*100\n",
    "    print('Accuracy {} on Unknown Data'.format(accuracy_percent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error on Known data = 0.3105022831050228\n",
      "Error on unknown data = 0.31105169340463457\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.mllib.tree import RandomForest, RandomForestModel\n",
    "\n",
    "def labelData(line):\n",
    "        probability = [float(row) for row in line.split(' ')]\n",
    "        return LabeledPoint(probability[0], probability[1:])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #sc = SparkContext(appName=\"PythonRandomForestClassificationExample\")\n",
    "   \n",
    "    NYT_data = sc.textFile(\"Desktop/SHEF/known_cleaned.txt\")\n",
    "    labeledData = NYT_data.map(labelData)\n",
    "    NYT_data_unknown = sc.textFile(\"Desktop/SHEF/unknown_cleaned.txt\")\n",
    "    unknown_labeledData = NYT_data_unknown.map(labelData)\n",
    "    \n",
    "    training, test = labeledData.randomSplit([0.6, 0.4])\n",
    "    \n",
    "    # Train a RandomForest model.\n",
    "    RandomForestmodel = RandomForest.trainClassifier(training, numClasses=4, categoricalFeaturesInfo={},\n",
    "                                         numTrees=1, featureSubsetStrategy=\"auto\",\n",
    "                                         impurity='gini', maxDepth=2, maxBins=25)\n",
    "\n",
    "    # Evaluate model on test instances and compute test error\n",
    "    prediction = RandomForestmodel.predict(test.map(lambda x: x.features))\n",
    "    labelsAndPredictions = test.map(lambda lp: lp.label).zip(prediction)\n",
    "    #print(labelsAndPredictions.collect())\n",
    "    Error = labelsAndPredictions.filter(lambda lp: lp[0] != lp[1]).count() / float(test.count())\n",
    "    print('Error on Known data = ' + str(Error))\n",
    "    \n",
    "    prediction = RandomForestmodel.predict(unknown_labeledData.map(lambda x: x.features))\n",
    "    labelsAndPredictions = unknown_labeledData.map(lambda lp: lp.label).zip(prediction)\n",
    "    #print(labelsAndPredictions.collect())\n",
    "    Error = labelsAndPredictions.filter(lambda lp: lp[0] != lp[1]).count() / float(unknown_labeledData.count())\n",
    "    print('Error on unknown data = ' + str(Error))\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on known data = 0.8611111111111112\n",
      "Accuracy on unknown data = 0.946524064171123\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "from pyspark.mllib.util import MLUtils\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "# $example off$\n",
    "\n",
    "from pyspark import SparkContext\n",
    "\n",
    "def labelData(line):\n",
    "        probability = [float(row) for row in line.split(' ')]\n",
    "        return LabeledPoint(probability[0], probability[1:])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #sc = SparkContext(appName=\"MultiClassMetricsExample\")\n",
    "\n",
    "    # Several of the methods available in scala are currently missing from pyspark\n",
    "    # $example on$\n",
    "    # Load training data in LIBSVM format\n",
    "    NYT_data = sc.textFile(\"Desktop/SHEF/known_cleaned.txt\")\n",
    "    labeledData = NYT_data.map(labelData)\n",
    "    NYT_data_unknown = sc.textFile(\"Desktop/SHEF/unknown_cleaned.txt\")\n",
    "    unknown_labeledData = NYT_data_unknown.map(labelData)\n",
    "    \n",
    "    training, test = labeledData.randomSplit([0.6, 0.4])\n",
    "    \n",
    "    training.cache()\n",
    "    # Run training algorithm to build the model\n",
    "    LogisticRegmodel = LogisticRegressionWithLBFGS.train(training, numClasses=4)\n",
    "\n",
    "    # Compute raw scores on the test set\n",
    "    predictions = test.map(lambda lp: (float(LogisticRegmodel.predict(lp.features)), lp.label))\n",
    "    \n",
    "    predictions_unknown = unknown_labeledData.map(lambda lp: (float(LogisticRegmodel.predict(lp.features)), lp.label))\n",
    "\n",
    "    # Instantiate metrics object\n",
    "    metrics = MulticlassMetrics(predictions)\n",
    "    metrics2 = MulticlassMetrics(predictions_unknown)\n",
    "\n",
    "    # Overall statistics\n",
    "    accuracy = metrics.precision()\n",
    "    accuracy_unknown = metrics2.precision()\n",
    "    print(\"Accuracy on known data = %s\" % accuracy)\n",
    "    print(\"Accuracy on unknown data = %s\" % accuracy_unknown)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# KMEANS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Known data: 0.8962966822359193\n",
      "Accuracy on unKnown data: 0.8962966822359193\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "import numpy as np\n",
    "from pyspark import SparkContext\n",
    "from pyspark.mllib.clustering import KMeans\n",
    "\n",
    "def ConvertToVector(line):\n",
    "    return np.array([float(x) for x in line.split(' ')])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    #sc = SparkContext(appName=\"PythonLogisticRegressionWithLBFGSExample\")\n",
    "    data = sc.textFile(\"Desktop/SHEF/known_cleaned.txt\")\n",
    "    data_unknown = sc.textFile(\"Desktop/SHEF/unknown_cleaned.txt\")\n",
    "    \n",
    "    convertedVectorData = data.map(ConvertToVector)\n",
    "    convertedVectorData_unknown = data_unknown.map(ConvertToVector)\n",
    "    \n",
    "    Kmeansmodel = KMeans.train(convertedVectorData, 4)\n",
    "    Kmeansmodel_unknown = KMeans.train(convertedVectorData_unknown, 4)\n",
    "    \n",
    "    print(\"Accuracy on Known data: \" + str(Kmeansmodel.computeCost(convertedVectorData)))\n",
    "    print(\"Accuracy on unKnown data: \" + str(Kmeansmodel.computeCost(convertedVectorData_unknown)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DECISION TREE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error on known data = 0.5691367631349948\n",
      "Mean Squared Error on unknown data = 0.31517259185832314\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from pyspark.mllib.tree import DecisionTree, DecisionTreeModel\n",
    "\n",
    "def labelData(line):\n",
    "        probability = [float(row) for row in line.split(' ')]\n",
    "        return LabeledPoint(probability[0], probability[1:])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    #sc = SparkContext(appName=\"PythonDecisionTreeRegressionExample\")\n",
    "\n",
    "    NYT_data = sc.textFile(\"Desktop/SHEF/known_cleaned.txt\")\n",
    "    labeledData = NYT_data.map(labelData)\n",
    "    NYT_data_unknown = sc.textFile(\"Desktop/SHEF/unknown_cleaned.txt\")\n",
    "    unknown_labeledData = NYT_data_unknown.map(labelData)\n",
    "    training, test = labeledData.randomSplit([0.6, 0.4])    \n",
    "\n",
    "    # Train a DecisionTree model.\n",
    "    DecisionTreemodel = DecisionTree.trainRegressor(training, categoricalFeaturesInfo={},\n",
    "                                        impurity='variance', maxDepth=5, maxBins=32)\n",
    "\n",
    "    # Evaluate model on test instances and compute test error\n",
    "    predictions = DecisionTreemodel.predict(test.map(lambda x: x.features))\n",
    "    labelsAndPredictions = test.map(lambda lp: lp.label).zip(predictions)\n",
    "    testMSE = labelsAndPredictions.map(lambda lp: (lp[0] - lp[1]) * (lp[0] - lp[1])).sum() /\\\n",
    "        float(test.count())\n",
    "    print('Mean Squared Error on known data = ' + str(testMSE))\n",
    "    \n",
    "    predictions = DecisionTreemodel.predict(unknown_labeledData.map(lambda x: x.features))\n",
    "    labelsAndPredictions = unknown_labeledData.map(lambda lp: lp.label).zip(predictions)\n",
    "    testMSE = labelsAndPredictions.map(lambda lp: (lp[0] - lp[1]) * (lp[0] - lp[1])).sum() /\\\n",
    "        float(unknown_labeledData.count())\n",
    "    print('Mean Squared Error on unknown data = ' + str(testMSE))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
